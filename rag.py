# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FN5Yl-zzonDznR_Io4z70qsyvqZ2C1I_

RAG model
"""

import os

if "COLAB_GPU" in os.environ:
    !pip install -U torch -qq > /dev/null 2>&1
    !pip install PyMuPDF -qq > /dev/null 2>&1
    !pip install pdfplumber -qq > /dev/null 2>&1
    !pip install python-docx -qq > /dev/null 2>&1
    !pip install docxtxt -qq > /dev/null 2>
    !pip install tqdm -qq > /dev/null 2>&1
    !pip install sentence-transformers -qq > /dev/null 2>&1
    !pip install accelerate -qq > /dev/null 2>&1
    !pip install bitsandbytes -qq > /dev/null 2>&1
    !pip install flash-attn --no-build-isolation -qq > /dev/null 2>&1

from google.colab import files
from tqdm.auto import tqdm
import ipywidgets as widgets
from IPython.display import display, clear_output
import random
import fitz  # PyMuPDF for PDF processing

# Global variable to store the uploaded file name
uploaded_file_name = None
pages_and_texts = []

# Function to handle file upload
def upload_pdf():
    global uploaded_file_name
    print("Please upload your PDF file:")
    uploaded = files.upload()
    for file_name in uploaded.keys():
        if file_name.lower().endswith('.pdf'):
            print(f"File '{file_name}' uploaded successfully.")
            uploaded_file_name = file_name
        else:
            print(f"'{file_name}' is not a PDF file. Skipping...")

# Function to clean extracted text
def text_formatter(text: str) -> str:
    return text.replace("\n", " ").strip()

# Function to process PDF and extract text
def open_and_read_pdf(pdf_path: str) -> list[dict]:
    try:
        doc = fitz.open(pdf_path)

        for page_number, page in tqdm(enumerate(doc, start=1), desc="Reading pages"):
            text = page.get_text()
            if text.strip():
                pages_and_texts.append({
                    "page_number": page_number,
                    "text": text_formatter(text)
                })
        doc.close()
        return pages_and_texts
    except Exception as e:
        print(f"An error occurred while reading the PDF: {e}")
        return []

# Function to handle button click
def on_button_click(b):
    clear_output()  # Clear the previous output
    upload_pdf()
    if uploaded_file_name:
        print(f"Processing the uploaded file: {uploaded_file_name}")
        pages_and_texts = open_and_read_pdf(uploaded_file_name)
        if pages_and_texts:
            print(f"Successfully processed {len(pages_and_texts)} pages.")

            # Sample 3 random pages
            num_samples = min(3, len(pages_and_texts))
            sampled_pages = random.sample(pages_and_texts, k=num_samples)
            print("\nRandom Samples:")
            for page in sampled_pages:
                print(f"Page {page['page_number']}: {page['text'][:200]}...\n")  # First 200 chars
        else:
            print("No text extracted from the PDF.")
    else:
        print("No valid PDF file was uploaded.")

# Create the upload button with an icon
upload_button = widgets.Button(
    description="Upload PDF",
    icon="upload",
    button_style="success"  # 'success' makes the button green
)

# Assign the button click event
upload_button.on_click(on_button_click)

# Display the button
display(upload_button)

import random
random.sample(pages_and_texts, k=3)

import pandas as pd
df = pd.DataFrame(pages_and_texts)
df.head()

df.describe().round(2)

"""# Further text processing (splitting pages into sentences)"""

from spacy.lang.en import English
nlp = English()
nlp.add_pipe("sentencizer")

for item in tqdm(pages_and_texts):
    item["sentences"] = list(nlp(item["text"]).sents)
    item["sentences"] = [str(sentence) for sentence in item["sentences"]]

    item["page_sentence_count_spacy"] = len(item["sentences"])

random.sample(pages_and_texts, k=1)

print(item["page_sentence_count_spacy"])

"""# Chunking our sentences together"""

num_sentence_chunk_size = 10
def split_list(input_list: list,
               slice_size: int) -> list[list[str]]:

    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]

for item in tqdm(pages_and_texts):
    item["sentence_chunks"] = split_list(input_list=item["sentences"],
                                         slice_size=num_sentence_chunk_size)
    item["num_chunks"] = len(item["sentence_chunks"])

random.sample(pages_and_texts, k=1)

"""# Splitting each chunk into its own item"""

import pandas as pd
import re
from tqdm.auto import tqdm

# Ensure pages_and_chunks is properly initialized
pages_and_chunks = []

# Populate pages_and_chunks from pages_and_texts
for item in tqdm(pages_and_texts, desc="Processing pages"):
    for sentence_chunk in item["sentence_chunks"]:
        chunk_dict = {}
        chunk_dict["page_number"] = item["page_number"]

        # Join sentences into a chunk
        joined_sentence_chunk = ". ".join(sentence_chunk).replace("  ", " ").strip()
        joined_sentence_chunk = re.sub(r'\.([A-Z])', r'. \1', joined_sentence_chunk)
        chunk_dict["sentence_chunk"] = joined_sentence_chunk

        # Add metadata
        chunk_dict["chunk_char_count"] = len(joined_sentence_chunk)
        chunk_dict["chunk_word_count"] = len(joined_sentence_chunk.split(" "))
        chunk_dict["chunk_token_count"] = len(joined_sentence_chunk) / 4  # Approx token count

        pages_and_chunks.append(chunk_dict)

# Convert to DataFrame
df = pd.DataFrame(pages_and_chunks)

# Check for missing columns
if "chunk_token_count" not in df.columns:
    print("Adding 'chunk_token_count' to the DataFrame...")
    df["chunk_token_count"] = df["sentence_chunk"].apply(lambda x: len(x) / 4 if isinstance(x, str) else 0)

# Check the number of rows and columns
print(f"DataFrame has {len(df)} rows and {len(df.columns)} columns.")
print(df.head())

# Filter and sample
min_token_length = 10
filtered_df = df[df["chunk_token_count"] <= min_token_length]

if not filtered_df.empty:
    for _, row in filtered_df.sample(1).iterrows():
        print(f'Chunk token count: {row["chunk_token_count"]} | Text: {row["sentence_chunk"]}')
else:
    print("No chunks meet the condition. Sampling is not possible.")

random.sample(pages_and_chunks, k=1)

chunk_dict

# Get stats about our chunks
df = pd.DataFrame(pages_and_chunks)
df.describe().round(2)

import random

if len(pages_and_chunks) > 0:
    random_sample = random.sample(pages_and_chunks, k=1)
    print(random_sample)
else:
    print("The list 'pages_and_chunks' is empty, so sampling is not possible.")

df = pd.DataFrame(pages_and_chunks)
df.describe().round(2)

print("Columns in DataFrame:", df.columns)

min_token_length = 10
filtered_df = df[df["chunk_token_count"] <= min_token_length]

if not filtered_df.empty:
    # Sample one row and print
    for _, row in filtered_df.sample(1).iterrows():
        print(f'Chunk token count: {row["chunk_token_count"]} | Text: {row["sentence_chunk"]}')
else:
    print("No chunks meet the condition. Sampling is not possible.")

print(df.columns)
print(df.head())

pages_and_chunks_over_min_token_len = df[df["chunk_token_count"] > min_token_length].to_dict(orient="records")
pages_and_chunks_over_min_token_len[:2]

"""# Embedding our text chunks"""

import os
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.vectorstores import Qdrant
from langchain_ollama import ChatOllama
from qdrant_client import QdrantClient
from langchain import PromptTemplate
from langchain.chains import RetrievalQA
import streamlit as st

class ChatbotManager:
    def __init__(
        self,
        model_name: str = "BAAI/bge-small-en",
        device: str = "cpu",
        encode_kwargs: dict = {"normalize_embeddings": True},
        llm_model: str = "llama3.2:3b",
        llm_temperature: float = 0.7,
        qdrant_url: str = "http://localhost:6333",
        collection_name: str = "vector_db",
    ):
        """
        Initializes the ChatbotManager with embedding models, LLM, and vector store.

        Args:
            model_name (str): The HuggingFace model name for embeddings.
            device (str): The device to run the model on ('cpu' or 'cuda').
            encode_kwargs (dict): Additional keyword arguments for encoding.
            llm_model (str): The local LLM model name for ChatOllama.
            llm_temperature (float): Temperature setting for the LLM.
            qdrant_url (str): The URL for the Qdrant instance.
            collection_name (str): The name of the Qdrant collection.
        """
        self.model_name = model_name
        self.device = device
        self.encode_kwargs = encode_kwargs
        self.llm_model = llm_model
        self.llm_temperature = llm_temperature
        self.qdrant_url = qdrant_url
        self.collection_name = collection_name

        # Initialize Embeddings
        self.embeddings = HuggingFaceBgeEmbeddings(
            model_name=self.model_name,
            model_kwargs={"device": self.device},
            encode_kwargs=self.encode_kwargs,
        )

        # Initialize Local LLM
        self.llm = ChatOllama(
            model=self.llm_model,
            temperature=self.llm_temperature,
            # Add other parameters if needed
        )

        # Define the prompt template
        self.prompt_template = """Use the following pieces of information to answer the user's question.
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context: {context}
Question: {question}

Only return the helpful answer. Answer must be detailed and well explained.
Helpful answer:
"""

        # Initialize Qdrant client
        self.client = QdrantClient(
            url=self.qdrant_url, prefer_grpc=False
        )

        # Initialize the Qdrant vector store
        self.db = Qdrant(
            client=self.client,
            embeddings=self.embeddings,
            collection_name=self.collection_name
        )

        # Initialize the prompt
        self.prompt = PromptTemplate(
            template=self.prompt_template,
            input_variables=['context', 'question']
        )

        # Initialize the retriever
        self.retriever = self.db.as_retriever(search_kwargs={"k": 1})

        # Define chain type kwargs
        self.chain_type_kwargs = {"prompt": self.prompt}

        # Initialize the RetrievalQA chain with return_source_documents=False
        self.qa = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.retriever,
            return_source_documents=False,  # Set to False to return only 'result'
            chain_type_kwargs=self.chain_type_kwargs,
            verbose=False
        )

    def get_response(self, query: str) -> str:
        """
        Processes the user's query and returns the chatbot's response.

        Args:
            query (str): The user's input question.

        Returns:
            str: The chatbot's response.
        """
        try:
            response = self.qa.run(query)
            return response  # 'response' is now a string containing only the 'result'
        except Exception as e:
            st.error(f"⚠️ An error occurred while processing your request: {e}")
            return "⚠️ Sorry, I couldn't process your request at the moment."

import sys
import contextlib
from sentence_transformers import SentenceTransformer

# Initialize the embedding model
embedding_model = SentenceTransformer(model_name_or_path="all-mpnet-base-v2")

# Ensure pages_and_chunks is already populated from the first block
# Filter chunks with sufficient token length
min_token_length = 10
pages_and_chunks_over_min_token_len = [
    chunk for chunk in pages_and_chunks if chunk.get("chunk_token_count", 0) > min_token_length
]

# Extract sentences from the filtered chunks
sentences = [chunk["sentence_chunk"] for chunk in pages_and_chunks_over_min_token_len]

# Check if we have extracted any sentences
if len(sentences) == 0:
    print("No sentences available for embedding. Check the token counts or data input.")
else:
    print(f"Total sentences available for embedding: {len(sentences)}")

    # Suppress output during embedding generation
    with contextlib.redirect_stdout(sys.stderr):
        # Generate embeddings silently
        embeddings = embedding_model.encode(sentences)

    # Create a dictionary of sentences and their embeddings
    embeddings_dict = dict(zip(sentences, embeddings))

    print("Embeddings have been generated successfully.")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# embedding_model.to("cpu")
# for item in tqdm(pages_and_chunks_over_min_token_len):
#     item["embedding"] = embedding_model.encode(item["sentence_chunk"])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Send the model to the GPU
# #embedding_model.to("cuda")
# 
# # Create embeddings one by one on the GPU
# #for item in tqdm(pages_and_chunks_over_min_token_len):
#     #item["embedding"] = embedding_model.encode(item["sentence_chunk"])

# Turn text chunks into a single list
text_chunks = [item["sentence_chunk"] for item in pages_and_chunks_over_min_token_len]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Embed all texts in batches
# text_chunk_embeddings = embedding_model.encode(text_chunks,
#                                                batch_size=32,
#                                                convert_to_tensor=True) # optional to return embeddings as tensor instead of array
# 
# text_chunk_embeddings

# Save embeddings to file
text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)
embeddings_df_save_path = "text_chunks_and_embeddings_df.csv"
text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)

# Import saved file and view
text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)
text_chunks_and_embedding_df_load.head()

import random

import torch
import numpy as np
import pandas as pd

device = "cuda" if torch.cuda.is_available() else "cpu"

# Import texts and embedding df
text_chunks_and_embedding_df = pd.read_csv("text_chunks_and_embeddings_df.csv")

# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)
text_chunks_and_embedding_df["embedding"] = text_chunks_and_embedding_df["embedding"].apply(lambda x: np.fromstring(x.strip("[]"), sep=" "))

# Convert texts and embedding df to list of dicts
pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient="records")

# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)
embeddings = torch.tensor(np.array(text_chunks_and_embedding_df["embedding"].tolist()), dtype=torch.float32).to(device)
embeddings.shape

text_chunks_and_embedding_df.head()

embeddings[0]

from sentence_transformers import util, SentenceTransformer

embedding_model = SentenceTransformer(model_name_or_path="all-mpnet-base-v2",
                                      device=device) # choose the device to load the model to

import torch
from sentence_transformers import util
from timeit import default_timer as timer  # Import the timer

# Query input
query = input("Enter your Question: ")
print(f"Query: {query}")

# Embed the query to the same numerical space as the text examples
query_embedding = embedding_model.encode(query, convert_to_tensor=True)

# Get similarity scores with the dot product
start_time = timer()
dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]
end_time = timer()

print(f"Time taken to get scores on {len(embeddings)} embeddings: {end_time - start_time:.5f} seconds.")

# Get the top-k results
top_k = 2
top_results_dot_product = torch.topk(dot_scores, k=top_k)

# Extract the indices and scores of the top results
top_indices = top_results_dot_product.indices.tolist()
top_scores = top_results_dot_product.values.tolist()

# Display the top results
print("\nTop Results:")
for i, (index, score) in enumerate(zip(top_indices, top_scores)):
    chunk = pages_and_chunks[index]

    # Safely get the text data
    text = chunk.get('sentence_chunk', "No text available")
    print(f"\nResult {i+1}:")
    print(f"Score: {score:.4f}")
    print(f"Text: {text}")

larger_embeddings = torch.randn(100*embeddings.shape[0], 768).to(device)
print(f"Embeddings shape: {larger_embeddings.shape}")

# Perform dot product across 168,000 embeddings
start_time = timer()
dot_scores = util.dot_score(a=query_embedding, b=larger_embeddings)[0]
end_time = timer()

print(f"Time take to get scores on {len(larger_embeddings)} embeddings: {end_time-start_time:.5f} seconds.")

# Define helper function to print wrapped text
import textwrap

def print_wrapped(text, wrap_length=80):
    wrapped_text = textwrap.fill(text, wrap_length)
    print(wrapped_text)

print(f"Query: '{query}'\n")
print("Results:")

# Loop through scores and indices from torch.topk
for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):
    print(f"Score: {score:.4f}")
    # Print relevant sentence chunk
    print("Text:")
    print_wrapped(pages_and_chunks[idx]["sentence_chunk"])

    # Get page number with a fallback if it doesn't exist
    page_number = pages_and_chunks[idx].get("page_number", "N/A")  # Replace 'page_number' with actual key if different
    print(f"Page number: {page_number}")
    print("\n")

